# Prerequisites
*.d

# Object files
*.o
*.ko
*.obj
*.elf

# Linker output
*.ilk
*.map
*.exp

# Precompiled Headers
*.gch
*.pch

# Libraries
*.lib
*.a
*.la
*.lo

# Shared objects (inc. Windows DLLs)
*.dll
*.so
*.so.*
*.dylib

# Executables
*.exe
*.out
*.app
*.i*86
*.x86_64
*.hex

# Debug files
*.dSYM/
*.su
*.idb
*.pdb

# Kernel Module Compile Results
*.mod*
*.cmd
.tmp_versions/
modules.order
Module.symvers
Mkfile.old
dkms.conf

# my ignores
build
.vscode
test.c
test.cpp
svmTest.c
svm.h
libsvm/
libsvm
.DS_Store
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <string.h>

#define MAX_ROWS 150
#define MAX_FEATURES 2 // Using two features: sepal length and petal length

typedef struct svm_node {
    int index;
    double value;
} svm_node;

typedef struct svm_problem {
    int l;                 // Number of data points
    double *y;             // Labels
    svm_node **x;          // Features
} svm_problem;

typedef struct svm_parameter {
    double C;              // Regularization parameter
    double eps;            // Stopping tolerance
} svm_parameter;

typedef struct svm_model {
    double *sv_coef;       // Support vector coefficients
    double rho;            // Intercept
    svm_node **SV;         // Support vectors
    int l;                 // Number of support vectors
} svm_model;

// Helper function: dot product of two vectors
double dot(const svm_node *px, const svm_node *py) {
    double sum = 0;
    while (px->index != -1 && py->index != -1) {
        if (px->index == py->index) {
            sum += px->value * py->value;
            px++;
            py++;
        } else if (px->index < py->index) {
            px++;
        } else {
            py++;
        }
    }
    return sum;
}

// Decision function for SVM
double decision_function(const svm_node *x, const svm_model *model) {
    double result = -model->rho;
    for (int i = 0; i < model->l; i++) {
        result += model->sv_coef[i] * dot(x, model->SV[i]);
    }
    return result;
}

// SVM prediction
double svm_predict(const svm_model *model, const svm_node *x) {
    double decision = decision_function(x, model);
    return decision > 0 ? 1.0 : -1.0;
}

// SVM training for a linear kernel
svm_model *svm_train(const svm_problem *prob, const svm_parameter *param) {
    int l = prob->l;
    double *alpha = calloc(l, sizeof(double));
    double *gradient = calloc(l, sizeof(double));
    svm_model *model = malloc(sizeof(svm_model));

    double step_size = 0.01;
    for (int iter = 0; iter < 1000; iter++) {
        for (int i = 0; i < l; i++) {
            double yi = prob->y[i];
            double gi = dot(prob->x[i], prob->x[i]) * alpha[i] + gradient[i];
            if (yi * gi < 1) {
                alpha[i] += step_size * (yi - gi);
                gradient[i] += step_size * yi;
            }
        }
    }

    // Count support vectors
    int sv_count = 0;
    for (int i = 0; i < l; i++) {
        if (alpha[i] > 1e-6) sv_count++;
    }

    model->l = sv_count;
    model->sv_coef = malloc(sv_count * sizeof(double));
    model->SV = malloc(sv_count * sizeof(svm_node *));
    int sv_index = 0;
    for (int i = 0; i < l; i++) {
        if (alpha[i] > 1e-6) {
            model->sv_coef[sv_index] = alpha[i] * prob->y[i];
            model->SV[sv_index] = prob->x[i];
            sv_index++;
        }
    }

    // Compute intercept (bias term)
    double rho = 0;
    for (int i = 0; i < sv_count; i++) {
        rho += model->sv_coef[i] * dot(model->SV[i], model->SV[i]);
    }
    model->rho = rho / sv_count;

    free(alpha);
    free(gradient);
    return model;
}

// Free the SVM model
void svm_free_model(svm_model *model) {
    free(model->sv_coef);
    free(model->SV);
    free(model);
}

// Load dataset from a CSV file
int load_dataset(const char *filename, svm_problem *prob, int max_rows) {
    FILE *file = fopen(filename, "r");
    if (!file) {
        perror("Failed to open file");
        return -1;
    }

    prob->l = 0;
    prob->y = malloc(max_rows * sizeof(double));
    prob->x = malloc(max_rows * sizeof(svm_node *));

    char line[256];
    fgets(line, sizeof(line), file); // Skip the header line

    while (fgets(line, sizeof(line), file) && prob->l < max_rows) {
        double sepal_length, sepal_width, petal_length, petal_width;
        int target;
        if (sscanf(line, "%lf,%lf,%lf,%lf,%d", &sepal_length, &sepal_width, &petal_length, &petal_width, &target) == 5) {
            if (target == 0 || target == 1) { // Binary classification
                prob->y[prob->l] = (target == 0) ? 1.0 : -1.0;
                prob->x[prob->l] = malloc(3 * sizeof(svm_node));
                prob->x[prob->l][0].index = 1;
                prob->x[prob->l][0].value = sepal_length;
                prob->x[prob->l][1].index = 2;
                prob->x[prob->l][1].value = petal_length;
                prob->x[prob->l][2].index = -1; // End of features
                prob->l++;
            }
        }
    }

    fclose(file);
    return prob->l;
}
// Calculate accuracy
double calculate_accuracy(const svm_model *model, const svm_problem *prob) {
    int correct = 0;
    for (int i = 0; i < prob->l; i++) {
        double prediction = svm_predict(model, prob->x[i]);
        if ((int)prediction == (int)prob->y[i]) {
            correct++;
        }
    }
    return (double)correct / prob->l * 100.0;
}

// Split dataset into training and testing sets
void split_dataset(svm_problem *full_prob, svm_problem *train_prob, svm_problem *test_prob) {
    int half = full_prob->l / 2; // Split 50-50

    // Allocate training set
    train_prob->l = half;
    train_prob->y = malloc(half * sizeof(double));
    train_prob->x = malloc(half * sizeof(svm_node *));
    for (int i = 0; i < half; i++) {
        train_prob->y[i] = full_prob->y[i];
        train_prob->x[i] = full_prob->x[i];
    }

    // Allocate testing set
    test_prob->l = full_prob->l - half;
    test_prob->y = malloc(test_prob->l * sizeof(double));
    test_prob->x = malloc(test_prob->l * sizeof(svm_node *));
    for (int i = half; i < full_prob->l; i++) {
        test_prob->y[i - half] = full_prob->y[i];
        test_prob->x[i - half] = full_prob->x[i];
    }
}

int main(void) {
    svm_problem full_prob, train_prob, test_prob;
    const char *filename = "irisV2.csv";

    // Load full dataset
    if (load_dataset(filename, &full_prob, MAX_ROWS) <= 0) {
        fprintf(stderr, "Failed to load dataset or no valid points found.\n");
        return EXIT_FAILURE;
    }

    // Split into training and testing sets
    split_dataset(&full_prob, &train_prob, &test_prob);

    // Parameters
    svm_parameter param;
    param.C = 1.0;
    param.eps = 1e-3;

    // Train the SVM
    svm_model *model = svm_train(&train_prob, &param);
    printf("Model trained successfully.\n");

    // Calculate accuracy on test set
    double accuracy = calculate_accuracy(model, &test_prob);
    printf("Test Accuracy: %.2f%%\n", accuracy);

    // Free resources
    svm_free_model(model);
    for (int i = 0; i < full_prob.l; i++) {
        free(full_prob.x[i]);
    }
    free(full_prob.x);
    free(full_prob.y);

    return 0;
}
